#!/usr/bin/env python3
"""
Comprehensive Galaxy Comparison Analysis for SDCG
==================================================

This script combines ALL available galaxy data sources for a thorough comparison:

1. Verified void dwarfs (38 galaxies)
2. Verified cluster dwarfs (30 galaxies)
3. Local Group dwarfs (25 galaxies with environment classification)
4. SPARC galaxies (175 with rotation curves)
5. ALFALFA dwarfs (with W50 â†’ V_rot conversion)
6. LITTLE THINGS (41 nearby dwarfs)

Goal: Maximum sample size for void vs cluster comparison

Author: CGC Analysis Pipeline
Date: February 3, 2026
"""

import numpy as np
import json
import os
import csv
from scipy import stats
from collections import defaultdict

PROJECT_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
DATA_DIR = os.path.join(PROJECT_DIR, "data")

# SDCG Prediction
SDCG_DELTA_V = 12.0  # km/s
SDCG_ERROR = 3.0  # km/s


# ============================================================================
# DATA LOADERS
# ============================================================================

def load_verified_dwarfs():
    """Load verified void and cluster dwarfs."""
    void_path = os.path.join(DATA_DIR, "dwarfs", "verified_void_dwarfs.json")
    cluster_path = os.path.join(DATA_DIR, "dwarfs", "verified_cluster_dwarfs.json")
    
    void_dwarfs = []
    cluster_dwarfs = []
    
    if os.path.exists(void_path):
        with open(void_path, 'r') as f:
            data = json.load(f)
        for g in data.get('galaxies', []):
            if g.get('v_rot') and g['v_rot'] > 0:
                void_dwarfs.append({
                    'name': g['name'],
                    'v_rot': g['v_rot'],
                    'v_rot_err': g.get('v_rot_err', 5.0),
                    'dist': g.get('dist', 20),
                    'source': g.get('source', 'Verified'),
                    'type': 'dwarf_irregular'
                })
    
    if os.path.exists(cluster_path):
        with open(cluster_path, 'r') as f:
            data = json.load(f)
        for g in data.get('galaxies', []):
            if g.get('v_rot') and g['v_rot'] > 0:
                cluster_dwarfs.append({
                    'name': g['name'],
                    'v_rot': g['v_rot'],
                    'v_rot_err': g.get('v_rot_err', 3.0),
                    'dist': g.get('dist', 16.5),
                    'source': g.get('source', 'Verified'),
                    'type': 'dE'
                })
    
    return void_dwarfs, cluster_dwarfs


def load_local_group():
    """Load Local Group dwarfs with environment classification."""
    lg_path = os.path.join(DATA_DIR, "dwarfs", "local_group_dwarfs.json")
    
    void_dwarfs = []
    cluster_dwarfs = []
    
    if os.path.exists(lg_path):
        with open(lg_path, 'r') as f:
            data = json.load(f)
        
        columns = data.get('columns', [])
        rows = data.get('data', [])
        
        # Find column indices
        name_idx = columns.index('Name') if 'Name' in columns else 0
        sigma_idx = columns.index('sigma_v_km_s') if 'sigma_v_km_s' in columns else 4
        sigma_err_idx = columns.index('sigma_v_err') if 'sigma_v_err' in columns else 5
        env_idx = columns.index('Environment') if 'Environment' in columns else -1
        dist_idx = columns.index('Distance_kpc') if 'Distance_kpc' in columns else 1
        
        for row in rows:
            name = row[name_idx]
            sigma_v = row[sigma_idx]
            sigma_err = row[sigma_err_idx]
            env = row[env_idx] if env_idx >= 0 else 'unknown'
            dist_kpc = row[dist_idx]
            
            # Convert dispersion to approximate rotation velocity
            # For pressure-supported dwarfs: V_rot ~ 2 * sigma_v
            v_rot = 2.0 * sigma_v
            v_rot_err = 2.0 * sigma_err
            
            entry = {
                'name': name,
                'v_rot': v_rot,
                'v_rot_err': v_rot_err,
                'sigma_v': sigma_v,
                'dist': dist_kpc / 1000.0,  # Convert to Mpc
                'source': 'McConnachie+2012',
                'type': 'dSph'
            }
            
            if env == 'void':
                void_dwarfs.append(entry)
            elif env == 'cluster':
                cluster_dwarfs.append(entry)
    
    return void_dwarfs, cluster_dwarfs


def load_little_things():
    """Load LITTLE THINGS catalog."""
    lt_path = os.path.join(DATA_DIR, "little_things", "little_things_catalog.json")
    
    void_dwarfs = []
    cluster_dwarfs = []
    field_dwarfs = []
    
    if os.path.exists(lt_path):
        with open(lt_path, 'r') as f:
            data = json.load(f)
        
        for g in data.get('galaxies', []):
            if g.get('v_rot') and g['v_rot'] > 0:
                entry = {
                    'name': g['name'],
                    'v_rot': g['v_rot'],
                    'v_rot_err': g.get('v_rot_err', 3.0),
                    'dist': g.get('dist', 5),
                    'source': 'LITTLE_THINGS',
                    'type': 'dIrr'
                }
                
                env = g.get('environment', 'field')
                if env == 'void':
                    void_dwarfs.append(entry)
                elif env == 'cluster':
                    cluster_dwarfs.append(entry)
                else:
                    field_dwarfs.append(entry)
    
    return void_dwarfs, cluster_dwarfs, field_dwarfs


def load_sparc_galaxies():
    """
    Load SPARC galaxies and classify by environment.
    Uses galaxy coordinates to determine environment.
    """
    sparc_path = os.path.join(DATA_DIR, "sparc", "sparc_data.mrt")
    
    if not os.path.exists(sparc_path):
        return [], [], []
    
    # Parse SPARC MRT format
    galaxies = []
    
    with open(sparc_path, 'r') as f:
        lines = f.readlines()
    
    # Find data start
    data_start = 0
    for i, line in enumerate(lines):
        if line.startswith('-----'):
            data_start = i + 1
            break
    
    # Parse line pairs
    i = data_start
    while i < len(lines) - 1:
        line1 = lines[i].rstrip()
        line2 = lines[i+1].rstrip() if i+1 < len(lines) else ''
        
        if not line1 or not line1[0].isalpha():
            i += 1
            continue
        
        combined = line1 + ' ' + line2
        parts = combined.split()
        
        if len(parts) < 15:
            i += 1
            continue
        
        try:
            name = parts[0]
            htype = int(parts[1]) if parts[1].isdigit() else 10
            dist = float(parts[2])
            inc = float(parts[5])
            
            # Find Vflat
            vflat = 0
            for j in range(14, min(22, len(parts)-1)):
                try:
                    val = float(parts[j])
                    err = float(parts[j+1])
                    if 15 < val < 400 and 0.5 < err < val/2:
                        vflat = val
                        break
                except:
                    continue
            
            if vflat > 0 and inc >= 25 and vflat < 120:  # Dwarf criterion
                galaxies.append({
                    'name': name,
                    'v_rot': vflat,
                    'v_rot_err': 5.0,
                    'dist': dist,
                    'inc': inc,
                    'hubble_type': htype,
                    'source': 'SPARC',
                    'type': 'late-type'
                })
            
            i += 2
        except:
            i += 1
    
    # Classify by environment (based on typical SPARC locations)
    # Most SPARC galaxies are field galaxies
    void_dwarfs = []
    cluster_dwarfs = []
    field_dwarfs = galaxies  # Default to field
    
    return void_dwarfs, cluster_dwarfs, field_dwarfs


def load_alfalfa_dwarfs():
    """Load ALFALFA dwarfs with W50 â†’ V_rot conversion."""
    alfalfa_path = os.path.join(DATA_DIR, "alfalfa", "alfalfa_a40.csv")
    
    if not os.path.exists(alfalfa_path):
        return [], [], []
    
    # Known voids and clusters for environment classification
    VOIDS = [
        ('Local_Void', 295.0, 5.0, 1, 25, 40.0),
        ('Lynx_Cancer', 130.0, 40.0, 10, 35, 25.0),
        ('CVn_Void', 190.0, 35.0, 3, 15, 15.0),
    ]
    
    CLUSTERS = [
        ('Virgo', 187.7, 12.4, 16.5, 8.0),
        ('Fornax', 54.6, -35.5, 19.0, 4.0),
        ('Coma', 195.0, 28.0, 100.0, 2.5),
    ]
    
    def angular_sep(ra1, dec1, ra2, dec2):
        ra1, dec1, ra2, dec2 = map(np.radians, [ra1, dec1, ra2, dec2])
        cos_sep = np.sin(dec1)*np.sin(dec2) + np.cos(dec1)*np.cos(dec2)*np.cos(ra1-ra2)
        return np.degrees(np.arccos(np.clip(cos_sep, -1, 1)))
    
    def classify_env(ra, dec, dist):
        for name, c_ra, c_dec, c_dist, c_rad in CLUSTERS:
            if angular_sep(ra, dec, c_ra, c_dec) < c_rad and abs(dist - c_dist)/c_dist < 0.3:
                return 'cluster', name
        for name, v_ra, v_dec, d_min, d_max, v_rad in VOIDS:
            if angular_sep(ra, dec, v_ra, v_dec) < v_rad and d_min < dist < d_max:
                return 'void', name
        return 'field', None
    
    void_dwarfs = []
    cluster_dwarfs = []
    field_dwarfs = []
    
    with open(alfalfa_path, 'r') as f:
        reader = csv.DictReader(f)
        
        for row in reader:
            try:
                w50 = float(row['W50'])
                w50_err = float(row.get('errW50', 10))
                dist_str = row.get('Dist', '')
                v_helio = float(row['Vhelio'])
                log_mhi = float(row.get('logMsun', 9.0))
                ra = float(row['RAdeg_HI'])
                dec = float(row['Decdeg_HI'])
                snr = float(row.get('SNR', 5))
                
                if not dist_str:
                    dist = v_helio / 70.0
                else:
                    dist = float(dist_str)
                
                # Dwarf criteria
                if w50 > 150 or log_mhi > 9.5 or dist > 100 or dist < 1 or snr < 6:
                    continue
                
                # W50 to V_rot (assume median inclination 60Â°)
                inc = 60.0
                sin_i = np.sin(np.radians(inc))
                sigma_turb = 10.0
                w_corr = np.sqrt(max(0, w50**2 - (2*sigma_turb)**2))
                v_rot = w_corr / (2 * sin_i)
                
                if v_rot > 120 or v_rot < 5:
                    continue
                
                env, env_name = classify_env(ra, dec, dist)
                
                entry = {
                    'name': row.get('Name', f"AGC{row.get('AGCNr', '')}"),
                    'v_rot': v_rot,
                    'v_rot_err': 15.0,  # Higher error due to assumed inclination
                    'w50': w50,
                    'dist': dist,
                    'log_mhi': log_mhi,
                    'source': 'ALFALFA',
                    'type': 'HI-dwarf'
                }
                
                if env == 'void':
                    void_dwarfs.append(entry)
                elif env == 'cluster':
                    cluster_dwarfs.append(entry)
                else:
                    field_dwarfs.append(entry)
                    
            except Exception as e:
                continue
    
    return void_dwarfs, cluster_dwarfs, field_dwarfs


# ============================================================================
# ANALYSIS FUNCTIONS
# ============================================================================

def remove_duplicates(galaxy_list):
    """Remove duplicate galaxies by name."""
    seen = set()
    unique = []
    for g in galaxy_list:
        name = g['name'].upper().replace(' ', '').replace('-', '').replace('_', '')
        if name not in seen:
            seen.add(name)
            unique.append(g)
    return unique


def create_matched_samples(void_list, cluster_list, method='random'):
    """Create matched samples with equal N."""
    n_match = min(len(void_list), len(cluster_list))
    
    if n_match == 0:
        return [], []
    
    np.random.seed(42)
    
    if method == 'random':
        void_idx = np.random.choice(len(void_list), size=n_match, replace=False)
        cluster_idx = np.random.choice(len(cluster_list), size=n_match, replace=False)
        return [void_list[i] for i in void_idx], [cluster_list[i] for i in cluster_idx]
    
    elif method == 'velocity_matched':
        # Sort by velocity and match
        void_sorted = sorted(void_list, key=lambda x: x['v_rot'])
        cluster_sorted = sorted(cluster_list, key=lambda x: x['v_rot'])
        
        void_idx = np.linspace(0, len(void_sorted)-1, n_match, dtype=int)
        cluster_idx = np.linspace(0, len(cluster_sorted)-1, n_match, dtype=int)
        
        return [void_sorted[i] for i in void_idx], [cluster_sorted[i] for i in cluster_idx]
    
    return void_list[:n_match], cluster_list[:n_match]


def compute_comprehensive_stats(void_sample, cluster_sample, n_bootstrap=10000):
    """Compute comprehensive statistics."""
    void_v = np.array([g['v_rot'] for g in void_sample])
    cluster_v = np.array([g['v_rot'] for g in cluster_sample])
    
    n_void = len(void_v)
    n_cluster = len(cluster_v)
    
    # Basic stats
    void_mean = np.mean(void_v)
    void_std = np.std(void_v, ddof=1)
    void_sem = void_std / np.sqrt(n_void)
    void_median = np.median(void_v)
    
    cluster_mean = np.mean(cluster_v)
    cluster_std = np.std(cluster_v, ddof=1)
    cluster_sem = cluster_std / np.sqrt(n_cluster)
    cluster_median = np.median(cluster_v)
    
    # Difference
    delta_mean = void_mean - cluster_mean
    delta_median = void_median - cluster_median
    delta_err = np.sqrt(void_sem**2 + cluster_sem**2)
    
    # Bootstrap
    boot_deltas = []
    for _ in range(n_bootstrap):
        v_boot = np.random.choice(void_v, size=n_void, replace=True)
        c_boot = np.random.choice(cluster_v, size=n_cluster, replace=True)
        boot_deltas.append(np.mean(v_boot) - np.mean(c_boot))
    
    boot_deltas = np.array(boot_deltas)
    boot_mean = np.mean(boot_deltas)
    boot_err = np.std(boot_deltas)
    ci_95 = np.percentile(boot_deltas, [2.5, 97.5])
    
    # Statistical tests
    t_stat, t_pval = stats.ttest_ind(void_v, cluster_v)
    u_stat, u_pval = stats.mannwhitneyu(void_v, cluster_v, alternative='greater')
    ks_stat, ks_pval = stats.ks_2samp(void_v, cluster_v)
    
    # Effect size (Cohen's d)
    pooled_std = np.sqrt((void_std**2 + cluster_std**2) / 2)
    cohens_d = delta_mean / pooled_std if pooled_std > 0 else 0
    
    # Significance
    sigma_from_zero = delta_mean / delta_err if delta_err > 0 else 0
    sigma_from_sdcg = abs(delta_mean - SDCG_DELTA_V) / np.sqrt(delta_err**2 + SDCG_ERROR**2)
    
    return {
        'n_void': n_void,
        'n_cluster': n_cluster,
        'void': {
            'mean': void_mean,
            'std': void_std,
            'sem': void_sem,
            'median': void_median,
            'min': void_v.min(),
            'max': void_v.max()
        },
        'cluster': {
            'mean': cluster_mean,
            'std': cluster_std,
            'sem': cluster_sem,
            'median': cluster_median,
            'min': cluster_v.min(),
            'max': cluster_v.max()
        },
        'delta': {
            'mean': delta_mean,
            'median': delta_median,
            'error': delta_err,
            'bootstrap_mean': boot_mean,
            'bootstrap_err': boot_err,
            'ci_95_low': ci_95[0],
            'ci_95_high': ci_95[1]
        },
        'tests': {
            't_stat': t_stat,
            't_pval': t_pval,
            'u_stat': u_stat,
            'u_pval': u_pval,
            'ks_stat': ks_stat,
            'ks_pval': ks_pval,
            'cohens_d': cohens_d
        },
        'significance': {
            'sigma_from_zero': sigma_from_zero,
            'sigma_from_sdcg': sigma_from_sdcg
        }
    }


def print_report(label, void_sample, cluster_sample, stats):
    """Print formatted analysis report."""
    
    print(f"\n{'='*75}")
    print(f"ANALYSIS: {label}")
    print('='*75)
    
    print(f"\nâ”Œ{'â”€'*73}â”")
    print(f"â”‚{'SAMPLE COMPOSITION':^73}â”‚")
    print(f"â”œ{'â”€'*73}â”¤")
    print(f"â”‚  Void galaxies:    {stats['n_void']:>5} (mean V_rot = {stats['void']['mean']:.1f} km/s)          â”‚")
    print(f"â”‚  Cluster galaxies: {stats['n_cluster']:>5} (mean V_rot = {stats['cluster']['mean']:.1f} km/s)          â”‚")
    print(f"â””{'â”€'*73}â”˜")
    
    d = stats['delta']
    print(f"\nâ”Œ{'â”€'*73}â”")
    print(f"â”‚{'VELOCITY DIFFERENCE (Void - Cluster)':^73}â”‚")
    print(f"â”œ{'â”€'*73}â”¤")
    print(f"â”‚  Î”v (mean)     = {d['mean']:+7.2f} Â± {d['error']:.2f} km/s                              â”‚")
    print(f"â”‚  Î”v (median)   = {d['median']:+7.2f} km/s                                        â”‚")
    print(f"â”‚  Bootstrap     = {d['bootstrap_mean']:+7.2f} Â± {d['bootstrap_err']:.2f} km/s                              â”‚")
    print(f"â”‚  95% CI        = [{d['ci_95_low']:+6.1f}, {d['ci_95_high']:+6.1f}] km/s                              â”‚")
    print(f"â””{'â”€'*73}â”˜")
    
    t = stats['tests']
    s = stats['significance']
    print(f"\nâ”Œ{'â”€'*73}â”")
    print(f"â”‚{'STATISTICAL SIGNIFICANCE':^73}â”‚")
    print(f"â”œ{'â”€'*73}â”¤")
    print(f"â”‚  t-test:         t = {t['t_stat']:6.2f}, p = {t['t_pval']:.2e}                           â”‚")
    print(f"â”‚  Mann-Whitney U: U = {t['u_stat']:6.0f}, p = {t['u_pval']:.2e}                           â”‚")
    print(f"â”‚  KS test:        D = {t['ks_stat']:6.3f}, p = {t['ks_pval']:.2e}                           â”‚")
    print(f"â”‚  Cohen's d:      {t['cohens_d']:+6.2f} ({'small' if abs(t['cohens_d'])<0.5 else 'medium' if abs(t['cohens_d'])<0.8 else 'large'} effect)                                   â”‚")
    print(f"â”‚                                                                         â”‚")
    print(f"â”‚  Significance from zero: {s['sigma_from_zero']:+.1f}Ïƒ                                        â”‚")
    print(f"â””{'â”€'*73}â”˜")
    
    print(f"\nâ”Œ{'â”€'*73}â”")
    print(f"â”‚{'SDCG COMPARISON':^73}â”‚")
    print(f"â”œ{'â”€'*73}â”¤")
    print(f"â”‚  SDCG Prediction: Î”v = +{SDCG_DELTA_V:.0f} Â± {SDCG_ERROR:.0f} km/s                                â”‚")
    print(f"â”‚  Observed:        Î”v = {d['mean']:+.1f} Â± {d['error']:.1f} km/s                                 â”‚")
    print(f"â”‚  Deviation:       {s['sigma_from_sdcg']:.1f}Ïƒ                                                   â”‚")
    
    if d['mean'] > 0:
        if s['sigma_from_sdcg'] < 1:
            status = "âœ“âœ“âœ“ EXCELLENT AGREEMENT"
        elif s['sigma_from_sdcg'] < 2:
            status = "âœ“âœ“  CONSISTENT"
        elif s['sigma_from_sdcg'] < 3:
            status = "âœ“   MARGINAL"
        else:
            status = "âœ—   TENSION"
    else:
        status = "âœ—   OPPOSITE SIGN"
    
    print(f"â”‚  Status:         {status:<40}â”‚")
    print(f"â””{'â”€'*73}â”˜")
    
    # Source breakdown
    print(f"\n  Sources in void sample:")
    sources = defaultdict(int)
    for g in void_sample:
        sources[g['source']] += 1
    for src, n in sorted(sources.items(), key=lambda x: -x[1]):
        print(f"    - {src}: {n}")
    
    print(f"\n  Sources in cluster sample:")
    sources = defaultdict(int)
    for g in cluster_sample:
        sources[g['source']] += 1
    for src, n in sorted(sources.items(), key=lambda x: -x[1]):
        print(f"    - {src}: {n}")


# ============================================================================
# MAIN
# ============================================================================

def main():
    print("="*75)
    print("COMPREHENSIVE GALAXY COMPARISON FOR SDCG")
    print("="*75)
    
    # Load all data sources
    print("\nğŸ“¥ Loading all available data sources...\n")
    
    # 1. Verified dwarfs
    v_void, v_cluster = load_verified_dwarfs()
    print(f"  Verified dwarfs:    {len(v_void)} void, {len(v_cluster)} cluster")
    
    # 2. Local Group
    lg_void, lg_cluster = load_local_group()
    print(f"  Local Group:        {len(lg_void)} void, {len(lg_cluster)} cluster")
    
    # 3. LITTLE THINGS
    lt_void, lt_cluster, lt_field = load_little_things()
    print(f"  LITTLE THINGS:      {len(lt_void)} void, {len(lt_cluster)} cluster, {len(lt_field)} field")
    
    # 4. SPARC
    sp_void, sp_cluster, sp_field = load_sparc_galaxies()
    print(f"  SPARC:              {len(sp_void)} void, {len(sp_cluster)} cluster, {len(sp_field)} field")
    
    # 5. ALFALFA
    al_void, al_cluster, al_field = load_alfalfa_dwarfs()
    print(f"  ALFALFA:            {len(al_void)} void, {len(al_cluster)} cluster, {len(al_field)} field")
    
    # =========================================================================
    # ANALYSIS 1: Verified + Local Group only (highest quality)
    # =========================================================================
    print("\n" + "="*75)
    print("ANALYSIS 1: HIGH-QUALITY SAMPLES (Verified + Local Group)")
    print("="*75)
    
    all_void_hq = remove_duplicates(v_void + lg_void)
    all_cluster_hq = remove_duplicates(v_cluster + lg_cluster)
    
    print(f"\nTotal after deduplication:")
    print(f"  Void:    {len(all_void_hq)}")
    print(f"  Cluster: {len(all_cluster_hq)}")
    
    void_matched, cluster_matched = create_matched_samples(all_void_hq, all_cluster_hq)
    
    if len(void_matched) >= 5:
        stats_hq = compute_comprehensive_stats(void_matched, cluster_matched)
        print_report("High-Quality Matched Samples", void_matched, cluster_matched, stats_hq)
    
    # =========================================================================
    # ANALYSIS 2: All sources combined (maximum N)
    # =========================================================================
    print("\n" + "="*75)
    print("ANALYSIS 2: ALL SOURCES COMBINED (Maximum Sample Size)")
    print("="*75)
    
    all_void = remove_duplicates(v_void + lg_void + lt_void + al_void)
    all_cluster = remove_duplicates(v_cluster + lg_cluster + lt_cluster + al_cluster)
    
    print(f"\nTotal after deduplication:")
    print(f"  Void:    {len(all_void)}")
    print(f"  Cluster: {len(all_cluster)}")
    
    void_matched_all, cluster_matched_all = create_matched_samples(all_void, all_cluster)
    
    if len(void_matched_all) >= 5:
        stats_all = compute_comprehensive_stats(void_matched_all, cluster_matched_all)
        print_report("All Sources Matched Samples", void_matched_all, cluster_matched_all, stats_all)
    
    # =========================================================================
    # ANALYSIS 3: Rotation curve only (exclude W50 conversions)
    # =========================================================================
    print("\n" + "="*75)
    print("ANALYSIS 3: ROTATION CURVES ONLY (Exclude W50 conversions)")
    print("="*75)
    
    # Filter out ALFALFA (W50-based)
    rc_void = [g for g in all_void if g['source'] != 'ALFALFA']
    rc_cluster = [g for g in all_cluster if g['source'] != 'ALFALFA']
    
    print(f"\nTotal with actual rotation curves:")
    print(f"  Void:    {len(rc_void)}")
    print(f"  Cluster: {len(rc_cluster)}")
    
    void_matched_rc, cluster_matched_rc = create_matched_samples(rc_void, rc_cluster)
    
    if len(void_matched_rc) >= 5:
        stats_rc = compute_comprehensive_stats(void_matched_rc, cluster_matched_rc)
        print_report("Rotation Curves Only", void_matched_rc, cluster_matched_rc, stats_rc)
    
    # =========================================================================
    # SUMMARY COMPARISON
    # =========================================================================
    print("\n" + "="*75)
    print("SUMMARY COMPARISON ACROSS ANALYSES")
    print("="*75)
    
    print(f"\n{'Analysis':<40} {'N':<8} {'Î”v (km/s)':<15} {'Significance':<12} {'SDCG Ïƒ':<10}")
    print("-"*75)
    
    if len(void_matched) >= 5:
        print(f"{'High-Quality (Verified+LG)':<40} {stats_hq['n_void']:<8} "
              f"{stats_hq['delta']['mean']:+.1f} Â± {stats_hq['delta']['error']:.1f}      "
              f"{stats_hq['significance']['sigma_from_zero']:+.1f}Ïƒ         "
              f"{stats_hq['significance']['sigma_from_sdcg']:.1f}Ïƒ")
    
    if len(void_matched_all) >= 5:
        print(f"{'All Sources Combined':<40} {stats_all['n_void']:<8} "
              f"{stats_all['delta']['mean']:+.1f} Â± {stats_all['delta']['error']:.1f}      "
              f"{stats_all['significance']['sigma_from_zero']:+.1f}Ïƒ         "
              f"{stats_all['significance']['sigma_from_sdcg']:.1f}Ïƒ")
    
    if len(void_matched_rc) >= 5:
        print(f"{'Rotation Curves Only':<40} {stats_rc['n_void']:<8} "
              f"{stats_rc['delta']['mean']:+.1f} Â± {stats_rc['delta']['error']:.1f}      "
              f"{stats_rc['significance']['sigma_from_zero']:+.1f}Ïƒ         "
              f"{stats_rc['significance']['sigma_from_sdcg']:.1f}Ïƒ")
    
    print("-"*75)
    print(f"{'SDCG Prediction':<40} {'---':<8} +12.0 Â± 3.0      ---          0.0Ïƒ")
    
    # Save results
    results = {
        'date': '2026-02-03',
        'analyses': {}
    }
    
    if len(void_matched) >= 5:
        results['analyses']['high_quality'] = {
            'n_void': stats_hq['n_void'],
            'n_cluster': stats_hq['n_cluster'],
            'delta_v': float(stats_hq['delta']['mean']),
            'delta_v_err': float(stats_hq['delta']['error']),
            'sigma_from_zero': float(stats_hq['significance']['sigma_from_zero']),
            'sigma_from_sdcg': float(stats_hq['significance']['sigma_from_sdcg'])
        }
    
    if len(void_matched_all) >= 5:
        results['analyses']['all_sources'] = {
            'n_void': stats_all['n_void'],
            'n_cluster': stats_all['n_cluster'],
            'delta_v': float(stats_all['delta']['mean']),
            'delta_v_err': float(stats_all['delta']['error']),
            'sigma_from_zero': float(stats_all['significance']['sigma_from_zero']),
            'sigma_from_sdcg': float(stats_all['significance']['sigma_from_sdcg'])
        }
    
    if len(void_matched_rc) >= 5:
        results['analyses']['rotation_curves'] = {
            'n_void': stats_rc['n_void'],
            'n_cluster': stats_rc['n_cluster'],
            'delta_v': float(stats_rc['delta']['mean']),
            'delta_v_err': float(stats_rc['delta']['error']),
            'sigma_from_zero': float(stats_rc['significance']['sigma_from_zero']),
            'sigma_from_sdcg': float(stats_rc['significance']['sigma_from_sdcg'])
        }
    
    output_path = os.path.join(DATA_DIR, "comprehensive_comparison_results.json")
    with open(output_path, 'w') as f:
        json.dump(results, f, indent=2)
    
    print(f"\n\nResults saved to: {output_path}")


if __name__ == "__main__":
    main()
